{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import ipytest\n",
    "from unittest.mock import Mock\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_cell(grid):\n",
    "    while True:\n",
    "        x = random.randint(0, grid.shape[0] - 1)\n",
    "        y = random.randint(0, grid.shape[1] - 1)\n",
    "        if grid[x, y] == 0:  # if the cell is not an obstacle\n",
    "            return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()  # list of strings, each string is a line of the file\n",
    "\n",
    "    grid = np.array([list(map(int, line.strip())) for line in lines[1:]])  # grid is a matrix of 1s and 0s\n",
    "    grid_reversed = np.flip(grid, axis=0)\n",
    "\n",
    "    return grid_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state, action, goal, grid, obstacle_punishment, wait_punishment):\n",
    "    goal_x, goal_y = goal\n",
    "    distance = np.linalg.norm(np.array([goal_x, goal_y]) - state)\n",
    "\n",
    "    if grid[state] == 1:\n",
    "        return obstacle_punishment  # obstacle\n",
    "    elif np.array_equal(state, goal):\n",
    "        return 100\n",
    "    elif action == (0, 0) and not np.array_equal(state, goal):\n",
    "        return wait_punishment  # wait action in non-goal state\n",
    "    else:\n",
    "        return -distance ** 3  # default case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_state(next_state, state, goal, grid):\n",
    "    if (0 <= next_state[0] < grid.shape[0]) and (0 <= next_state[1] < grid.shape[1]) and not np.array_equal(state, goal) and not grid[state] == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action, action_probabilities, grid, goal):\n",
    "    # each action has 3 possible outcomes\n",
    "    action_mapping = {\n",
    "        (-1, 1): [(-1, 0), (-1, 1), (0, 1)],\n",
    "        (0, 1): [(-1, 1), (0, 1), (1, 1)],\n",
    "        (1, 1): [(0, 1), (1, 1), (1, 0)],\n",
    "        (-1, 0): [(-1, -1), (-1, 0), (-1, 1)],\n",
    "        (0, 0): [(0, 0), (0, 0), (0, 0)],\n",
    "        (1, 0): [(1, 1), (1, 0), (1, -1)],\n",
    "        (-1, -1): [(0, -1), (-1, -1), (-1, 0)],\n",
    "        (0, -1): [(1, -1), (0, -1), (-1, -1)],\n",
    "        (1, -1): [(0, 1), (1, -1), (-1, 0)]\n",
    "    }\n",
    "\n",
    "    possible_actions = action_mapping[action]\n",
    "    # np.random.choice must receive a 1-dim list as first argument, or a scalar (and then it converts it to a list [0 .. scalar-1])\n",
    "    actual_action_index = np.random.choice(len(possible_actions), p=action_probabilities)\n",
    "    actual_action = possible_actions[actual_action_index]\n",
    "    next_state = (state[0] + actual_action[0], state[1] + actual_action[1])\n",
    "\n",
    "    # i move if next state is within bounds and i am not on an obstacle\n",
    "    if is_valid_state(next_state, state, goal, grid):\n",
    "        return next_state\n",
    "    else:\n",
    "        return state  # no movement if next state is out of bounds or i am currently on an obstacle or on the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i know how to get the next state given one\n",
    "def choose_action(state, actions, q_table, epsilon, montecarlo, policy):\n",
    "    action_idx = None\n",
    "    if montecarlo:\n",
    "        action_idx = policy[state[0], state[1]]\n",
    "    else:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            # explore (get a random action)\n",
    "            action_idx = random.randint(0, len(actions) - 1)\n",
    "        else:\n",
    "            # exploit (get the best action yet, argmax return index)\n",
    "            action_idx = np.argmax(q_table[state[0], state[1]])\n",
    "\n",
    "    return actions[action_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table_td(state, action_index, next_state, reward, q_table, gamma, alpha):\n",
    "    best_q_value = np.max(q_table[next_state[0], next_state[1]])\n",
    "    previous_q_value = q_table[state[0], state[1], action_index]\n",
    "    increment = reward + gamma * best_q_value - previous_q_value\n",
    "    q_table[state[0], state[1], action_index] += alpha * increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1 (complete) episode\n",
    "def generate_episode(goal, grid, q_table, params, montecarlo, policy):\n",
    "    episode = {\"states\": [], \"actions\": [], \"rewards\": []}\n",
    "    state = get_random_cell(grid)  # each episode has a random starting point\n",
    "\n",
    "    while not np.array_equal(state, goal) and len(episode[\"states\"]) < params['max_episode_length']:\n",
    "        # choose the best action with \"epsilon-greedy policy\"\n",
    "        action = choose_action(state, params['actions'], q_table, params['epsilon'], montecarlo, policy)\n",
    "\n",
    "        # apply action and observe next state and reward\n",
    "        next_state = get_next_state(state, action, params['action_probabilities'], grid, goal)\n",
    "        reward = get_reward(state, action, goal, grid, params['obstacle_punishment'], params['wait_punishment'])\n",
    "\n",
    "        episode[\"states\"].append(state)\n",
    "        episode[\"actions\"].append(action)\n",
    "        episode[\"rewards\"].append(reward)\n",
    "\n",
    "        if not montecarlo:\n",
    "            # update q_table\n",
    "            action_index = params['actions'].index(action)\n",
    "            update_q_table_td(state, action_index, next_state, reward, q_table, params['gamma'], params['alpha'])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after generating all the episodes, i can get the best policy from the q_table:\n",
    "def get_policy(grid, goal, q_table):\n",
    "    policy = np.zeros_like(grid, dtype=int)\n",
    "\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            if not np.array_equal((i, j), goal):\n",
    "                # in state (i,j), choose the action with the highest q-value\n",
    "                policy[i, j] = np.argmax(q_table[i, j])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(goal, grid, actions, policy):\n",
    "    fig, ax = plt.subplots()\n",
    "    # draw arrows representing the action in each cell\n",
    "    U = np.zeros_like(grid, dtype=float)\n",
    "    V = np.zeros_like(grid, dtype=float)\n",
    "\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            if not np.array_equal((i, j), goal):\n",
    "                action = actions[policy[i, j]]\n",
    "                U[i, j] = action[1]\n",
    "                V[i, j] = action[0]\n",
    "            if grid[i, j] == 1:\n",
    "                # draw a black square on obstacle cell\n",
    "                ax.fill([j, j, j+1, j+1], [i, i+1, i+1, i], \"k\")\n",
    "\n",
    "    X, Y = np.meshgrid(np.arange(0.5, grid.shape[1], 1), np.arange(0.5, grid.shape[0], 1))\n",
    "    ax.quiver(X, Y, U, V)\n",
    "    # draw a red dot on the goal cell\n",
    "    ax.scatter(goal[1] + 0.5, goal[0] + 0.5, color=\"r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i can put everything together:\n",
    "def q_learning(goal, grid, params):\n",
    "    # initialize q_table with zeros\n",
    "    q_table = np.zeros((grid.shape[0], grid.shape[1], 9))\n",
    "\n",
    "    # lists to store total rewards and episode lengths\n",
    "    rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for _ in tqdm(range(params['episodes_number']), desc=\"Episode\", colour=\"red\"):\n",
    "        episode = generate_episode(goal, grid, q_table, params, False, None)\n",
    "        rewards.append(np.sum(episode[\"rewards\"]))\n",
    "        episode_lengths.append(len(episode[\"states\"]))\n",
    "\n",
    "    policy = get_policy(grid, goal, q_table)\n",
    "    if params['plot_map']:\n",
    "        plot_policy(goal, grid, params['actions'], policy)\n",
    "    return policy, rewards, episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table_montecarlo(racum, state, rewards, action_index, gamma, policy, q_table):\n",
    "    utility = np.sum(gamma ** np.arange(len(rewards)) * rewards)\n",
    "    racum[state[0], state[1], action_index].append(utility)\n",
    "    q_table[state[0], state[1], action_index] = np.mean(racum[state[0], state[1], action_index])\n",
    "    policy[state[0], state[1]] = np.argmax(q_table[state[0], state[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def montecarlo(goal, grid, params):\n",
    "    rewards = []\n",
    "    episode_lengths = []\n",
    "    policy = np.full_like(grid, 4)\n",
    "\n",
    "    # initialize q_table with zeros\n",
    "    q_table = np.zeros((grid.shape[0], grid.shape[1], 9))\n",
    "\n",
    "    # initialize racum with empty lists\n",
    "    racum = np.empty((grid.shape[0], grid.shape[1], 9), dtype=object)\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            for a in range(9):\n",
    "                racum[i, j, a] = []\n",
    "\n",
    "    for e in tqdm(range(params['episodes_number']), colour=\"blue\", desc=\"Episode\"):\n",
    "        episode = generate_episode(goal, grid, q_table, params, True, policy)\n",
    "\n",
    "        # initialize first visit tables\n",
    "        first_visit_table = np.full((grid.shape[0], grid.shape[1], 9), True)\n",
    "        # algorithm excution\n",
    "\n",
    "        for i, (state, action) in enumerate(zip(episode['states'], episode['actions'])):\n",
    "            if params[\"every_visit_montecarlo\"] or first_visit_table[state[0], state[1], params[\"actions\"].index(action)]:\n",
    "                first_visit_table[state[0], state[1], params[\"actions\"].index(action)] = False\n",
    "                update_q_table_montecarlo(racum, state, episode['rewards'][i:], params[\"actions\"].index(action), params['gamma'], policy, q_table)\n",
    "\n",
    "        rewards.append(np.sum(episode['rewards']))\n",
    "        episode_lengths.append(len(episode['states']))\n",
    "\n",
    "    if params['plot_map']:\n",
    "        plot_policy(goal, grid, params['actions'], policy)\n",
    "    return policy, rewards, episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithms(algorithms, params):\n",
    "\n",
    "    grid = read_input(params['file_path'])\n",
    "\n",
    "    # initialize lists to store results\n",
    "    total_rewards = {name: [] for name in algorithms.keys()}\n",
    "    total_lengths = {name: [] for name in algorithms.keys()}\n",
    "\n",
    "    for run in tqdm(range(params['runs']), desc=\"Runs\"):\n",
    "\n",
    "        # generate random initial state and goal (the same for all algorithms)\n",
    "        goal = get_random_cell(grid)\n",
    "\n",
    "        for name, algorithm in algorithms.items():\n",
    "            # run algorithm (the first object returned is the policy, which is not needed here)\n",
    "            _, rewards, lengths = algorithm(goal, grid, params)\n",
    "\n",
    "            # accumulate results\n",
    "            total_rewards[name].append(rewards)\n",
    "            total_lengths[name].append(lengths)\n",
    "\n",
    "    avg_rewards = {name: np.mean(total_rewards[name], axis=0) for name in algorithms.keys()}\n",
    "    avg_lengths = {name: np.mean(total_lengths[name], axis=0) for name in algorithms.keys()}\n",
    "\n",
    "    return avg_rewards, avg_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(avg_rewards, avg_lengths):\n",
    "\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    colors = {\"q_learning\": \"red\", \"montecarlo\": \"blue\"}\n",
    "\n",
    "    for name in avg_rewards.keys():\n",
    "        axs[0].plot(avg_rewards[name], label=name, color=colors[name])\n",
    "        axs[1].plot(avg_lengths[name], label=name, color=colors[name])\n",
    "\n",
    "    axs[0].set_xlabel(\"Run\")\n",
    "    axs[0].set_ylabel(\"Average Reward\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].set_xlabel(\"Run\")\n",
    "    axs[1].set_ylabel(\"Average Episode Length\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliders():\n",
    "    sliders = {\n",
    "        'episodes_number_slider': widgets.IntSlider(min=1, max=100000, step=1, value=500, description='Episodes Number'),\n",
    "        'max_episode_length_slider': widgets.IntSlider(min=1, max=10000, step=1, value=500, description='Max Episode Length'),\n",
    "        'runs_slider': widgets.IntSlider(min=1, max=100, step=1, value=1, description='Runs'),\n",
    "        'obstacle_punishment_slider': widgets.FloatSlider(min=-1000, max=0, step=1, value=-500, description='Obstacle Punishment'),\n",
    "        'wait_punishment_slider': widgets.FloatSlider(min=-1000, max=0, step=1, value=-500, description='Wait Punishment'),\n",
    "        'alpha_slider': widgets.FloatSlider(min=0, max=1, step=0.01, value=0.5, description='Alpha'),\n",
    "        'gamma_slider': widgets.FloatSlider(min=0, max=1, step=0.01, value=0.5, description='Gamma'),\n",
    "        'epsilon_slider': widgets.FloatSlider(min=0, max=1, step=0.01, value=0.5, description='Epsilon'),\n",
    "        'plot_map': widgets.Checkbox(value=False, description='Plot Map'),\n",
    "        'every_visit_montecarlo': widgets.Checkbox(value=False, description='Every visit Montecarlo')\n",
    "    }\n",
    "    return sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probabilities = [0.05, 0.9, 0.05]\n",
    "file_path = \"../resources/small_map.txt\"\n",
    "actions = [(-1, 1), (0, 1), (1, 1),\n",
    "           (-1, 0), (0, 0), (1, 0),\n",
    "           (-1, -1), (0, -1), (1, -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_play_button_clicked(b, sliders):\n",
    "    clear_output(wait=True)\n",
    "    # get the values from the sliders\n",
    "    params = {\n",
    "        'file_path': file_path,\n",
    "        'actions': actions,\n",
    "        'action_probabilities': action_probabilities,\n",
    "        'episodes_number': sliders['episodes_number_slider'].value,\n",
    "        'max_episode_length': sliders['max_episode_length_slider'].value,\n",
    "        'runs': sliders['runs_slider'].value,\n",
    "        'obstacle_punishment': sliders['obstacle_punishment_slider'].value,\n",
    "        'wait_punishment': sliders['wait_punishment_slider'].value,\n",
    "        'alpha': sliders['alpha_slider'].value,\n",
    "        'gamma': sliders['gamma_slider'].value,\n",
    "        'epsilon': sliders['epsilon_slider'].value,\n",
    "        'plot_map': sliders['plot_map'].value,\n",
    "        'every_visit_montecarlo': sliders['every_visit_montecarlo'].value\n",
    "    }\n",
    "\n",
    "    algorithms = {\"q_learning\": q_learning, \"montecarlo\": montecarlo}\n",
    "    avg_rewards, avg_lengths = compare_algorithms(algorithms, params)\n",
    "    plot_comparison(avg_rewards, avg_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_panel():\n",
    "    sliders = create_sliders()\n",
    "    for slider in sliders.values():\n",
    "        display(slider)\n",
    "\n",
    "    play_button = widgets.Button(description=\"Play\")\n",
    "    play_button.on_click(lambda b: on_play_button_clicked(b, sliders))\n",
    "    return play_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811a887d67804be2bd9ae243ab1464dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=500, description='Episodes Number', max=100000, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9264a276e4964c5b9e673952bb016d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=500, description='Max Episode Length', max=10000, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdb5931247b41ff8cba0301334febaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=1, description='Runs', min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3fc5f6152e4f53b1b74008e0fea639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=-500.0, description='Obstacle Punishment', max=0.0, min=-1000.0, step=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515b8465d638414eba06a6b7a86daea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=-500.0, description='Wait Punishment', max=0.0, min=-1000.0, step=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367838ec16ed45729ef1975701cb2665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.5, description='Alpha', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d642bc75d36f402b93e71a31cc40e880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.5, description='Gamma', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672dbfdaedfc414c956221e3a2c62828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.5, description='Epsilon', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6546f92b7e345ceb8af6140723584c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Plot Map')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4dc95e711444ac492a7432312693389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Every visit Montecarlo')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50573ff061f5412bbd35260fe8dbcfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Play', style=ButtonStyle())"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_panel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ========================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipytest.autoconfig()\n",
    "@pytest.fixture # function to create a mock for the sliders (default values independent from the real sliders)\n",
    "def sliders():\n",
    "    return {\n",
    "        'episodes_number_slider': Mock(value=100),\n",
    "        'max_episode_length_slider': Mock(value=1000),\n",
    "        'runs_slider': Mock(value=10),\n",
    "        'obstacle_punishment_slider': Mock(value=-100),\n",
    "        'wait_punishment_slider': Mock(value=-1),\n",
    "        'alpha_slider': Mock(value=0.5),\n",
    "        'gamma_slider': Mock(value=0.9),\n",
    "        'epsilon_slider': Mock(value=0.1),\n",
    "        'plot_map': Mock(value=True),\n",
    "        'every_visit_montecarlo': Mock(value=False)\n",
    "    }\n",
    "\n",
    "def test_on_play_button_clicked(sliders):\n",
    "    # create a mock for the compare_algorithms function\n",
    "    mock_compare_algorithms = Mock()\n",
    "\n",
    "    # mock the return values to be two dictionaries\n",
    "    mock_return_value1 = {\"q_learning\": [1, 2, 3], \"montecarlo\": [4, 5, 6]}\n",
    "    mock_return_value2 = {\"q_learning\": [7, 8, 9], \"montecarlo\": [10, 11, 12]}\n",
    "\n",
    "    # set the return value of the mock function\n",
    "    mock_compare_algorithms.return_value = (mock_return_value1, mock_return_value2)\n",
    "\n",
    "    # replace the original function with the mock\n",
    "    original_compare_algorithms = globals()['compare_algorithms']\n",
    "    globals()['compare_algorithms'] = mock_compare_algorithms\n",
    "\n",
    "    # create a mock for the plot_comparison function\n",
    "    mock_plot_comparison = Mock()\n",
    "\n",
    "    # replace the original function with the mock\n",
    "    original_plot_comparison = globals()['plot_comparison']\n",
    "    globals()['plot_comparison'] = mock_plot_comparison\n",
    "\n",
    "    try:\n",
    "        # call the on_play_button_clicked function\n",
    "        on_play_button_clicked(None, sliders)\n",
    "\n",
    "        # check that the mock function was called with the correct parameters\n",
    "        expected_params = {\n",
    "            'file_path': file_path,\n",
    "            'actions': actions,\n",
    "            'action_probabilities': action_probabilities,\n",
    "            'episodes_number': sliders['episodes_number_slider'].value,\n",
    "            'max_episode_length': sliders['max_episode_length_slider'].value,\n",
    "            'runs': sliders['runs_slider'].value,\n",
    "            'obstacle_punishment': sliders['obstacle_punishment_slider'].value,\n",
    "            'wait_punishment': sliders['wait_punishment_slider'].value,\n",
    "            'alpha': sliders['alpha_slider'].value,\n",
    "            'gamma': sliders['gamma_slider'].value,\n",
    "            'epsilon': sliders['epsilon_slider'].value,\n",
    "            'plot_map': sliders['plot_map'].value,\n",
    "            'every_visit_montecarlo': sliders['every_visit_montecarlo'].value\n",
    "        }\n",
    "\n",
    "        mock_compare_algorithms.assert_called_once_with({\"q_learning\": q_learning, \"montecarlo\": montecarlo}, expected_params)\n",
    "        mock_plot_comparison.assert_called_once_with(mock_return_value1, mock_return_value2)\n",
    "\n",
    "    finally:\n",
    "        # Restore the original functions\n",
    "        globals()['compare_algorithms'] = original_compare_algorithms\n",
    "        globals()['plot_comparison'] = original_plot_comparison\n",
    "\n",
    "ipytest.run('-sv')  # s for capturing stdout, v for verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
